{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd5bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk import stem\n",
    "stemmer = stem.PorterStemmer()\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "from collections import Counter\n",
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "#!pip install PRAW\n",
    "import numpy as np\n",
    "import praw\n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64212ec",
   "metadata": {},
   "source": [
    "# The basics of lemmatising and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a03896",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"The quick brown fox jumped over the lazy dogs. The community of fowl sang in the dawn. The blind mice retreated before the feral felines. The geese flew north to escape the wolves\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6925a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(sent)\n",
    "stems = [stemmer.stem(i) for i in words]\n",
    "lemmas = [lemmatizer.lemmatize(i) for i in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wuthering_heights.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274e438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.encode('ascii', 'ignore')\n",
    "text = text.decode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d532627",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(text.splitlines())\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b78963",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609ad90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [lemmatizer.lemmatize(i) for i in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a21b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [i for i in lemmas if i not in stops and i not in punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a123bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.Series(Counter(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be5204",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f76109",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(counts, kind = 'kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b09ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = [len(i) for i in counts.index], y = [i for i in counts.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9530a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621d2257",
   "metadata": {},
   "source": [
    "# Using APIs to obtain data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22820ad2",
   "metadata": {},
   "source": [
    "## [Tracking the UK's journey to carbon zero](https://data.climateemergency.uk/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed1b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://data.climateemergency.uk/api/searchterms/'\n",
    "\n",
    "URL2 = \"https://data.climateemergency.uk/api/commitments/\"\n",
    "\n",
    "PARAMS = {'min_results':1}\n",
    "\n",
    "r = requests.get(url = URL, params = PARAMS)\n",
    "\n",
    "r = r.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431802ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "r['results'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55451b32",
   "metadata": {},
   "source": [
    "## [The Project Gutenberg API](https://gutendex.com/books/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68466db",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_g = {'topic':'love'}\n",
    "\n",
    "gut = 'https://gutendex.com/books/'\n",
    "\n",
    "rg = r = requests.get(url = gut, params = params_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cc30fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54495594",
   "metadata": {},
   "source": [
    "## The Reddit API\n",
    "\n",
    "1. Register for a Reddit account\n",
    "2. Go to https://www.reddit.com/prefs/apps and create a script app.\n",
    "3. Fill out the relevant fields, putting http://localhost:8080 for the URL fields\n",
    "4. Record the client_id (top left) and secret. The user_agent field can be arbitrary.\n",
    "5. The auth credentials are as follows:Â \n",
    "   `reddit = praw.Reddit(user_agent='YOURUSERAGENT',\n",
    "   client_id='YOURCLIENTID', client_secret=\"SECRET\",\n",
    "   username='USERNAME', password='PASSWORD')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(user_agent='VAD',\n",
    "                     client_id='eCo_TWE_BA_zFA', client_secret=\"1gsqXgMrZBoQBVYf40hgtvMS_Ro\",\n",
    "                     username='textureai', password='7233190s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b4e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(submission_id): ## submission_id can be URL or submission ID\n",
    "    try:\n",
    "        submission = reddit.submission(url = submission_id)\n",
    "    except:\n",
    "        submission = reddit.submission(submission_id)\n",
    "    title = submission.title\n",
    "    submission.comments.replace_more() ## loads new page if cooments are multipage\n",
    "    text = [i.body for i in submission.comments]\n",
    "    score = [i.score for i in submission.comments]\n",
    "    user = [i.author for i in submission.comments]\n",
    "    date = [datetime.datetime.fromtimestamp(i.created) for i in submission.comments]\n",
    "    df = pd.DataFrame()\n",
    "    df['text'] = text\n",
    "    df['datetime'] = date\n",
    "    df['score'] = score\n",
    "    df['subreddit'] = submission.subreddit\n",
    "    df['redditor'] = user\n",
    "    df['type'] = 'comment'\n",
    "    df['title'] = title\n",
    "    df = df.sort_values('score', ascending = False).reset_index(drop = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = submission('https://www.reddit.com/r/AskReddit/comments/10orx1j/what_movies_soundtrack_is_an_absolute_banger/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf0a30",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Write functions for interacting with the Reddit API that retrieve comments by (a) subreddit aand (b) by specific Reddit user and saves the result as a dataframe.\n",
    "2. Identify a topic of interest to you and retrieve the top 1k comments on that subreddit.\n",
    "3. Create a new column in your dataframe and that consists of the tokenized and lemmatized comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d37554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
